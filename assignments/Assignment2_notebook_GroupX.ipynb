{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dong2Yo/PredcitiveAnalysis_BusinessProfessional/blob/main/assignments/Assignment2_notebook_GroupX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as8aGwYOTTCt"
      },
      "outputs": [],
      "source": [
        "# import the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbdQ5GIjTTDG"
      },
      "source": [
        "# Assignment 2 - Classification\n",
        "\n",
        "For this assignment, you'll need to perform a classification on a dataset, as well as do some prep work on the data.\n",
        "\n",
        "The exact steps of what you need to do are flexible and up to you to some degree, however you should consider some of the important things we've mentioned recently, such as:\n",
        "<ul>\n",
        "<li> Is the target balanced?\n",
        "<li> Are there missing or erroneous values?\n",
        "<li> Are there categorical or numerical features?\n",
        "<li> Is there colinearity?\n",
        "<li> Are there outliers?\n",
        "<li> Should we normalize?\n",
        "<li> Do the distributions of the features give any indication that some may need work?\n",
        "</ul>\n",
        "\n",
        "Basically, the data is in the original, potentially dirty, format, and you need to find what should be cleaned, and do the cleaning. There is not one \"right\" answer to what has to be done, and you'll probably need to do it with at least something of an iterative process - find an issue, correct it, check the data, repeat.\n",
        "\n",
        "<b>The target that we are predicting is the loan_status. </b>\n",
        "\n",
        "## Grading\n",
        "\n",
        "The grades will be broken down as follows:\n",
        "<ul>\n",
        "<li> <b>30%</b> - a working predictive model with a reasonable level of accuracy.\n",
        "    <ul>\n",
        "    <li> For the accuracy part, it will not be explicitly graded, but compared to all the others. If you're in the same general range, that's good - if yours is drastically less accurate (or, I guess more accurate), then I'll adjust. There won't be a comparison of \"this person is 72.3% and this person is only 71.8% accurate, they fail.\"\n",
        "    <li> This data is larger than most of the small sample sets, so random variations due to train-test splits shouldn't be too bad. (If you're a keener you could loop)\n",
        "    <li> I will use the F1 score as the accuracy metric.\n",
        "    </ul>\n",
        "<li> <b>40%</b> - a clear and readable description of what steps you took to prepare the data, and a brief not on the rationale behind it. Did you do a log transformation to a skewed feature, did you remove outliers, did you remove a feature that has a lot of missing values? Please put this somewhere obvious and readable, consider the goal of the assignment to explain your process to me.\n",
        "    <ul>\n",
        "    <li> E.g. \"The target data was imbalanced, so I tried several resampling methods and chose the one with the highest accuracy of the resulting model\", \"the feature X had the same value for 95% of records, so I dropped it\".\n",
        "    <li> In this, please also state if you see a group that appears to be a good credit risk, and a group that is a bad credit risk, and indicate the evidence showing that. Please do this other than the most simplistic way - more money = better credit. If there are no such groups, state why you think this. This will likely be about 3 - 5 statements or points, you should provide evidence from the data, but it does not need to be an essay. (You may want to consider this question after you're pretty much done with the data prep and modelling.)\n",
        "    </ul>\n",
        "<li> <b>30%</b> - allowing your model to be \"deployed\". At the bottom of this file there is a small block of code to load in some test data (that I have), and calculate your accuracy. Your contribution to this part is to have a model that is ready to make predictions. Some specifics to consider:\n",
        "    <ul>\n",
        "    <li> The test data will be in exactly the same format as the dataset you're given. So any steps that you took to prepare your data for modelling will need to be mirroed here, so the new  data can be predicted. The easiest way to do this is to use a pipeline, but it is up to you. Remember the model only accepts data that is in a certain format - the one that you had the data in when it was trained, so when making predictions you need to make sure that is true.\n",
        "    <li> Since I'm providing test data, and your task is to just create a model, think about how that might impact your train-test splitting, both as you're developing and for the final product.\n",
        "    <li> Once the model is trained it should predict any data that is in the right format, so I should be able to provide any proper dataset, click run on that testing cell, and get predictions. I will not be doing anything that will purposefully make this harder or trickier, like including data that has errors or is in the wrong format, I just split the entire set of data, reserved part of it to test accuracy, and gave the rest to you. Things that you've changed (e.g. dropping a column, one-hot encoding) do need to be replicated at some point before the model can accept the new data to predict. In general we <b>do</b> want to check in our processing that our input is valid, this is kind of a junior introduction to that.\n",
        "    </ul>\n",
        "</ul"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here."
      ],
      "metadata": {
        "id": "yu5vWiz_6Zfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCZeR7ZHTTDU"
      },
      "source": [
        "## Testing The Accuracy on Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1P5AFQbTTDW"
      },
      "source": [
        "Assuming the variable is new_y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doRo1AaOTTDZ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Load the new data from the provided online URL\n",
        "url = 'https://raw.githubusercontent.com/DATA3320/DATA3320_1231/main/data/new_data_w_target.csv'\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# perform the exact steps you have done to prepossess the data here with the x feature.\n",
        "\n",
        "x =\n",
        "\n",
        "\n",
        "new_y_true = data['loan_status']\n",
        "\n",
        "# Compare predictions with the actual target values\n",
        "accuracy_new_data = accuracy_score(new_y_true, new_y_pred)\n",
        "print(f'Accuracy on the new data: {accuracy_new_data}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrV-cfe6TTDc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}